{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai-agents python-dotenv -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Agent Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm here and ready to help! How about you?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agent creation \n",
    "from agents import Agent, Runner\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Basic Agent\",\n",
    "    instructions = \"You are a helpful assistant.\",\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "result = await Runner.run(agent, 'Hi, How are you?')\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't skeletons fight each other? \\n\\nThey don't have the guts!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agent 2 example \n",
    "\n",
    "joke_agent = Agent(\n",
    "    name = \"Joke Agent\",\n",
    "    instructions = \"You are a funny assistant. Your jobs is to tell a funny joke.\",\n",
    "    model = \"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "joke_agent_result = await Runner.run(joke_agent, 'Tell me a joke')\n",
    "joke_agent_result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ہڈیاں ایک دوسرے سے کیوں نہیں لڑتیں؟\n",
      "\n",
      "کیونکہ ان میں حوصلہ نہیں ہوتا!\n"
     ]
    }
   ],
   "source": [
    "# Agent 3 example\n",
    "\n",
    "language_translator_agent = Agent(\n",
    "    name = \"Language Translator Agent\",\n",
    "    instructions = \"You are a translator. Your job is to translate given text into urdu.\",\n",
    "    model = \"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "language_translator_agent_result = await Runner.run(language_translator_agent, f\"translate this joke: {joke_agent_result.final_output} into urdu\")\n",
    "print(language_translator_agent_result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured output using Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Chicken Biryani Recipe\\n\\n#### **Ingredients:**\\n\\n**For the Chicken Marinade:**\\n- 1.5 lbs (700g) chicken, cut into pieces (bone-in pieces recommended)\\n- 1 cup plain yogurt\\n- 2 large onions, thinly sliced\\n- 4 cloves garlic, minced\\n- 1-inch piece ginger, grated\\n- 2-3 green chilies, slit (adjust to taste)\\n- 1 tablespoon red chili powder\\n- 1 teaspoon cumin powder\\n- 1 teaspoon coriander powder\\n- 1/2 teaspoon turmeric powder\\n- 1 tablespoon garam masala\\n- Salt to taste\\n- Juice of 1 lemon\\n- A handful of fresh mint leaves, chopped\\n- A handful of fresh cilantro (coriander leaves), chopped\\n\\n**For the Biryani:**\\n- 2 cups basmati rice\\n- 4 cups water (for boiling rice)\\n- 4 tablespoons ghee (clarified butter) or cooking oil\\n- 1-2 bay leaves\\n- 4-5 whole cloves\\n- 4-5 green cardamom pods\\n- 1-2 cinnamon sticks\\n- 1 star anise (optional)\\n- Saffron strands (optional, soaked in 2 tablespoons warm milk)\\n- Fried onions (for garnish)\\n- Additional mint and cilantro for garnish\\n\\n#### **Instructions:**\\n\\n**1. Marinate the Chicken (Time: 30 mins - 1 hour)**\\n   - In a large bowl, combine yogurt, garlic, ginger, chili powder, cumin, coriander, turmeric, garam masala, salt, lemon juice, mint, and cilantro.\\n   - Add the chicken pieces to the marinade, making sure they are well-coated.\\n   - Cover and let it marinate for at least 30 minutes or up to 1 hour in the refrigerator.\\n\\n**2. Prepare the Rice (Time: 20 mins)**\\n   - Rinse the basmati rice under cold water until the water runs clear to remove excess starch. Soak in water for 30 minutes.\\n   - After soaking, drain the rice.\\n   - In a large pot, bring 4 cups of water to a boil. Add the soaked rice and cook until al dente, about 5-6 minutes.\\n   - Drain the rice and set aside.\\n\\n**3. Cook the Chicken (Time: 15-20 mins)**\\n   - In a heavy-bottomed pot or pressure cooker, heat the ghee or oil over medium heat.\\n   - Add the sliced onions and sauté until golden brown. Reserve some for garnish.\\n   - Add the marinated chicken (without excess marinade) to the pot and cook for about 10-12 minutes, stirring occasionally, until the chicken is well-cooked and the spices are fragrant.\\n\\n**4. Layer the Biryani (Time: 5 mins)**\\n   - Once the chicken is cooked, turn the heat to low. Spread the partially cooked rice evenly over the chicken.\\n   - If using, drizzle saffron milk over the rice for color and flavor.\\n   - Cover the pot with a tight lid. You can seal the edges with dough if desired for better steam retention.\\n   - Cook on low heat for about 20-25 minutes until the rice is fully cooked and has absorbed all flavors (this process is known as ‘dum’ cooking).\\n\\n**5. Serve: (Total Time: 15 mins)**\\n   - Gently fluff the biryani with a fork, mixing the chicken and rice lightly.\\n   - Garnish with fried onions, additional mint, and cilantro.\\n   - Serve hot with raita (yogurt sauce) or salad.\\n\\n### **Total Time: Approximately 1.5 - 2 hours** (including marinating time)\\n\\nEnjoy your delicious homemade Chicken Biryani!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agents import Agent, Runner\n",
    "\n",
    "recipe_agent = Agent(\n",
    "    name = \"Recipe Agent\",\n",
    "    instructions = \"You are an expert chef. Your job is to provide a recipe for the given dish in detail like the recipe ingredients instructions how to make time in min for making that food.\",\n",
    "    model = \"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "result = await Runner.run(recipe_agent, \"give me a recipe for chicken biryani\")\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recipe(name='Chicken Biryani', ingredients=['2 cups Basmati rice', '500 grams chicken, cut into pieces', '1 large onion, thinly sliced', '2 tomatoes, chopped', '1/2 cup plain yogurt', '1/4 cup chopped fresh cilantro', '1/4 cup chopped fresh mint', '4 cups water', '3 tablespoons cooking oil or ghee', '2 teaspoons ginger-garlic paste', '1 teaspoon turmeric powder', '2 teaspoons biryani masala', '1 teaspoon cumin seeds', '4-5 green chilies, slit', '3-4 whole cloves', '2-3 green cardamoms', '1 small stick of cinnamon', 'Salt to taste', 'Saffron strands (optional)'], time='60 minutes', instructions=['Rinse the Basmati rice in cold water until the water runs clear. Soak it in water for 30 minutes, then drain.', 'In a large pot, heat oil or ghee over medium heat. Add cumin seeds, cloves, cardamom, and cinnamon stick. Sauté for a minute until fragrant.', 'Add the sliced onions and fry until golden brown.', 'Add the ginger-garlic paste and green chilies. Stir and sauté for 2-3 minutes until raw smell disappears.', 'Add the chicken pieces and sauté for 5-7 minutes until they are browned.', 'Stir in the chopped tomatoes, turmeric powder, biryani masala, and salt. Cook until tomatoes soften and oil separates from the mixture.', 'Add the yogurt, stir well, and let it cook covered for about 10-15 minutes until the chicken is cooked through.', 'In another pot, bring 4 cups of water to a boil. Add in the soaked and drained rice. Cook until the rice is 70% cooked (it should still have a bite to it). Drain the rice.', 'Layer the partially cooked rice over the chicken mixture in the pot. Sprinkle cilantro and mint leaves on top. If using saffron, dissolve it in 2 tablespoons of warm milk or water and drizzle it over the rice.', 'Cover the pot with a tight-fitting lid. Cook on low heat (dum) for about 20 minutes. You can place a heavy pot or tava (griddle) under the pot to avoid direct contact with the flame.', 'Once done, fluff the biryani gently with a fork. Serve hot with raita or salad.'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from agents import Agent, Runner\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    ingredients: list[str]\n",
    "    time: str\n",
    "    instructions: list[str]\n",
    "    \n",
    "\n",
    "recipe_agent = Agent(\n",
    "    name = \"Recipe Agent\",\n",
    "    instructions = \"You are an expert chef. Your job is to provide a recipe for the given dish in detail like the recipe ingredients instructions how to make time in min for making that food.\",\n",
    "    model = \"gpt-4o-mini\",\n",
    "    output_type = Recipe\n",
    ")\n",
    "\n",
    "result = await Runner.run(recipe_agent, \"give me a recipe for chicken biryani\")\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Chicken Biryani' ingredients=['2 cups basmati rice', '500 grams chicken, cut into pieces', '1 large onion, thinly sliced', '2 tomatoes, chopped', '1/2 cup plain yogurt', '4 cups water', '4 tablespoons biryani masala', '2 bay leaves', '4 green cardamom pods', '5 cloves', '1 cinnamon stick', '4 tablespoons oil or ghee', 'Salt to taste', 'Fresh cilantro and mint for garnishing', 'Saffron strands (optional)'] instructions='1. Rinse the basmati rice under cold water until the water runs clear. Soak the rice in water for 30 minutes, then drain.  \\n2. In a large pot, heat oil or ghee over medium heat. Add onions and sauté until golden brown.  \\n3. Add the chicken pieces and cook until they turn white.  \\n4. Stir in the chopped tomatoes and cook until soft.  \\n5. Add yogurt and biryani masala, mixing well. Cook until the chicken is tender and the oil separates from the mixture.  \\n6. In another pot, bring 4 cups of water to a boil. Add soaked and drained rice along with bay leaves, cardamom pods, cloves, cinnamon stick, and salt. Cook the rice until about 70% done, then drain.  \\n7. Layer half the partially cooked rice over the chicken mixture in the pot.  \\n8. If using, dissolve saffron in a tablespoon of warm water and drizzle over the rice. Then add the remaining rice on top.  \\n9. Cover the pot with a tight-fitting lid and cook on low heat for 25-30 minutes (this is the ‘dum’ process).  \\n10. Once done, take off the lid and fluff the biryani with a fork. Garnish with fresh cilantro and mint before serving. Enjoy your delicious Chicken Biryani!' cooking_time=60\n"
     ]
    }
   ],
   "source": [
    "# Structured output example\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    ingredients: list[str]\n",
    "    instructions: str\n",
    "    cooking_time: int\n",
    "    \n",
    "recipe_agent = Agent(\n",
    "    name = \"Recipe Agent\",\n",
    "    instructions = \"You are a recipe generator. Your job is to generate a recipe.\",\n",
    "    model = \"gpt-4o-mini\",\n",
    "    output_type=Recipe\n",
    ")\n",
    "recipe_agent_result = await Runner.run(recipe_agent, \"Generate a recipe for a biryani.\")\n",
    "print(recipe_agent_result.final_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Tool in Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of now, the current president of the United States is Joe Biden. He took office on January 20, 2021.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Built in Tool\n",
    "from agents import Agent, Runner, WebSearchTool\n",
    "\n",
    "web_search_tool = WebSearchTool()\n",
    "\n",
    "\n",
    "web_search_agent = Agent(\n",
    "    name = \"Web Search Agent\",\n",
    "    instructions = \"You are an expert web searcher. Your job is to search the relevant information from the web based on user query.\",\n",
    "    model = \"gpt-4o-mini\",\n",
    "    # tools = [web_search_tool]\n",
    ")\n",
    "\n",
    "\n",
    "result = await Runner.run(\n",
    "    web_search_agent,\n",
    "    \"Who is the current president of the United States?\",\n",
    ")\n",
    "\n",
    "result.final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************3u0A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}. (request_id: req_31e0b5a28925cb472f5a68ca53473cfc)\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************3u0A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe temperature in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is 25 degrees Celsius.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m get_tem_agent \u001b[38;5;241m=\u001b[39m Agent(\n\u001b[1;32m     10\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemperature Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     instructions \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an expert weather agent. Your job is to provide the temperature of the given city and use the tool to give proper response.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     tools \u001b[38;5;241m=\u001b[39m [get_temperature],\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(get_tem_agent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the temperature in New York?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m result\u001b[38;5;241m.\u001b[39mfinal_output\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/agents/run.py:218\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[0m\n\u001b[1;32m    213\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    215\u001b[0m )\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_turn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 218\u001b[0m     input_guardrail_results, turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_input_guardrails(\n\u001b[1;32m    220\u001b[0m             starting_agent,\n\u001b[1;32m    221\u001b[0m             starting_agent\u001b[38;5;241m.\u001b[39minput_guardrails\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;241m+\u001b[39m (run_config\u001b[38;5;241m.\u001b[39minput_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[1;32m    223\u001b[0m             copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[1;32m    224\u001b[0m             context_wrapper,\n\u001b[1;32m    225\u001b[0m         ),\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_turn(\n\u001b[1;32m    227\u001b[0m             agent\u001b[38;5;241m=\u001b[39mcurrent_agent,\n\u001b[1;32m    228\u001b[0m             all_tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[1;32m    229\u001b[0m             original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[1;32m    230\u001b[0m             generated_items\u001b[38;5;241m=\u001b[39mgenerated_items,\n\u001b[1;32m    231\u001b[0m             hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[1;32m    232\u001b[0m             context_wrapper\u001b[38;5;241m=\u001b[39mcontext_wrapper,\n\u001b[1;32m    233\u001b[0m             run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    234\u001b[0m             should_run_agent_start_hooks\u001b[38;5;241m=\u001b[39mshould_run_agent_start_hooks,\n\u001b[1;32m    235\u001b[0m             tool_use_tracker\u001b[38;5;241m=\u001b[39mtool_use_tracker,\n\u001b[1;32m    236\u001b[0m             previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[1;32m    237\u001b[0m         ),\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_turn(\n\u001b[1;32m    241\u001b[0m         agent\u001b[38;5;241m=\u001b[39mcurrent_agent,\n\u001b[1;32m    242\u001b[0m         all_tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m         previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[1;32m    251\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/agents/run.py:757\u001b[0m, in \u001b[0;36mRunner._run_single_turn\u001b[0;34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m ItemHelpers\u001b[38;5;241m.\u001b[39minput_to_new_input_list(original_input)\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mextend([generated_item\u001b[38;5;241m.\u001b[39mto_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[0;32m--> 757\u001b[0m new_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_new_response(\n\u001b[1;32m    758\u001b[0m     agent,\n\u001b[1;32m    759\u001b[0m     system_prompt,\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    761\u001b[0m     output_schema,\n\u001b[1;32m    762\u001b[0m     all_tools,\n\u001b[1;32m    763\u001b[0m     handoffs,\n\u001b[1;32m    764\u001b[0m     context_wrapper,\n\u001b[1;32m    765\u001b[0m     run_config,\n\u001b[1;32m    766\u001b[0m     tool_use_tracker,\n\u001b[1;32m    767\u001b[0m     previous_response_id,\n\u001b[1;32m    768\u001b[0m )\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_single_step_result_from_response(\n\u001b[1;32m    771\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[1;32m    772\u001b[0m     original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    781\u001b[0m     tool_use_tracker\u001b[38;5;241m=\u001b[39mtool_use_tracker,\n\u001b[1;32m    782\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/agents/run.py:916\u001b[0m, in \u001b[0;36mRunner._get_new_response\u001b[0;34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id)\u001b[0m\n\u001b[1;32m    913\u001b[0m model_settings \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mresolve(run_config\u001b[38;5;241m.\u001b[39mmodel_settings)\n\u001b[1;32m    914\u001b[0m model_settings \u001b[38;5;241m=\u001b[39m RunImpl\u001b[38;5;241m.\u001b[39mmaybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[0;32m--> 916\u001b[0m new_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[1;32m    917\u001b[0m     system_instructions\u001b[38;5;241m=\u001b[39msystem_prompt,\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    919\u001b[0m     model_settings\u001b[38;5;241m=\u001b[39mmodel_settings,\n\u001b[1;32m    920\u001b[0m     tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[1;32m    921\u001b[0m     output_schema\u001b[38;5;241m=\u001b[39moutput_schema,\n\u001b[1;32m    922\u001b[0m     handoffs\u001b[38;5;241m=\u001b[39mhandoffs,\n\u001b[1;32m    923\u001b[0m     tracing\u001b[38;5;241m=\u001b[39mget_model_tracing_impl(\n\u001b[1;32m    924\u001b[0m         run_config\u001b[38;5;241m.\u001b[39mtracing_disabled, run_config\u001b[38;5;241m.\u001b[39mtrace_include_sensitive_data\n\u001b[1;32m    925\u001b[0m     ),\n\u001b[1;32m    926\u001b[0m     previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[1;32m    927\u001b[0m )\n\u001b[1;32m    929\u001b[0m context_wrapper\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39madd(new_response\u001b[38;5;241m.\u001b[39musage)\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/agents/models/openai_responses.py:76\u001b[0m, in \u001b[0;36mOpenAIResponsesModel.get_response\u001b[0;34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled\u001b[38;5;241m=\u001b[39mtracing\u001b[38;5;241m.\u001b[39mis_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_response(\n\u001b[1;32m     77\u001b[0m             system_instructions,\n\u001b[1;32m     78\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     79\u001b[0m             model_settings,\n\u001b[1;32m     80\u001b[0m             tools,\n\u001b[1;32m     81\u001b[0m             output_schema,\n\u001b[1;32m     82\u001b[0m             handoffs,\n\u001b[1;32m     83\u001b[0m             previous_response_id,\n\u001b[1;32m     84\u001b[0m             stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     85\u001b[0m         )\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _debug\u001b[38;5;241m.\u001b[39mDONT_LOG_MODEL_DATA:\n\u001b[1;32m     88\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM responded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/agents/models/openai_responses.py:242\u001b[0m, in \u001b[0;36mOpenAIResponsesModel._fetch_response\u001b[0;34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, stream)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling LLM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with input:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson\u001b[38;5;241m.\u001b[39mdumps(list_input,\u001b[38;5;250m \u001b[39mindent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrevious response id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious_response_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mresponses\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    243\u001b[0m     previous_response_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(previous_response_id),\n\u001b[1;32m    244\u001b[0m     instructions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(system_instructions),\n\u001b[1;32m    245\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlist_input,\n\u001b[1;32m    247\u001b[0m     include\u001b[38;5;241m=\u001b[39mconverted_tools\u001b[38;5;241m.\u001b[39mincludes,\n\u001b[1;32m    248\u001b[0m     tools\u001b[38;5;241m=\u001b[39mconverted_tools\u001b[38;5;241m.\u001b[39mtools,\n\u001b[1;32m    249\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(model_settings\u001b[38;5;241m.\u001b[39mtemperature),\n\u001b[1;32m    250\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(model_settings\u001b[38;5;241m.\u001b[39mtop_p),\n\u001b[1;32m    251\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(model_settings\u001b[38;5;241m.\u001b[39mtruncation),\n\u001b[1;32m    252\u001b[0m     max_output_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(model_settings\u001b[38;5;241m.\u001b[39mmax_tokens),\n\u001b[1;32m    253\u001b[0m     tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[1;32m    254\u001b[0m     parallel_tool_calls\u001b[38;5;241m=\u001b[39mparallel_tool_calls,\n\u001b[1;32m    255\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    256\u001b[0m     extra_headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_HEADERS, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(model_settings\u001b[38;5;241m.\u001b[39mextra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[1;32m    257\u001b[0m     extra_query\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mextra_query,\n\u001b[1;32m    258\u001b[0m     extra_body\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mextra_body,\n\u001b[1;32m    259\u001b[0m     text\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    260\u001b[0m     store\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(model_settings\u001b[38;5;241m.\u001b[39mstore),\n\u001b[1;32m    261\u001b[0m     reasoning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(model_settings\u001b[38;5;241m.\u001b[39mreasoning),\n\u001b[1;32m    262\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(model_settings\u001b[38;5;241m.\u001b[39mmetadata),\n\u001b[1;32m    263\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/resources/responses/responses.py:1529\u001b[0m, in \u001b[0;36mAsyncResponses.create\u001b[0;34m(self, input, model, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1501\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1528\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response \u001b[38;5;241m|\u001b[39m AsyncStream[ResponseStreamEvent]:\n\u001b[0;32m-> 1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1530\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/responses\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1531\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1532\u001b[0m             {\n\u001b[1;32m   1533\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1534\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1535\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m: include,\n\u001b[1;32m   1536\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m\"\u001b[39m: instructions,\n\u001b[1;32m   1537\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_output_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_output_tokens,\n\u001b[1;32m   1538\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1539\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1540\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_response_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: previous_response_id,\n\u001b[1;32m   1541\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning,\n\u001b[1;32m   1542\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1543\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1544\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1545\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1546\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text,\n\u001b[1;32m   1547\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1548\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1549\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1550\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncation\u001b[39m\u001b[38;5;124m\"\u001b[39m: truncation,\n\u001b[1;32m   1551\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1552\u001b[0m             },\n\u001b[1;32m   1553\u001b[0m             response_create_params\u001b[38;5;241m.\u001b[39mResponseCreateParamsStreaming\n\u001b[1;32m   1554\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[1;32m   1555\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params\u001b[38;5;241m.\u001b[39mResponseCreateParamsNonStreaming,\n\u001b[1;32m   1556\u001b[0m         ),\n\u001b[1;32m   1557\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1558\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1559\u001b[0m         ),\n\u001b[1;32m   1560\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mResponse,\n\u001b[1;32m   1561\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1562\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ResponseStreamEvent],\n\u001b[1;32m   1563\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1742\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1729\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1730\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1738\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1739\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1740\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1741\u001b[0m     )\n\u001b[0;32m-> 1742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1549\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1546\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1548\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1549\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************3u0A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# Custom Tool\n",
    "from agents import Agent, Runner, function_tool\n",
    "\n",
    "@function_tool\n",
    "def get_temperature(city: str) -> str:\n",
    "    return f\"The temperature in {city} is 25 degrees Celsius.\"\n",
    "\n",
    "\n",
    "get_tem_agent = Agent(\n",
    "    name = \"Temperature Agent\",\n",
    "    instructions = \"You are an expert weather agent. Your job is to provide the temperature of the given city and use the tool to give proper response.\",\n",
    "    model= \"gpt-4o-mini\",\n",
    "    tools = [get_temperature],\n",
    ")\n",
    "\n",
    "result = await Runner.run(get_tem_agent, \"What is the temperature in New York?\")\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Karachi is sunny, with a current temperature of 70°F.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, function_tool\n",
    "\n",
    "@function_tool\n",
    "def getTemperature(city: str) -> str:\n",
    "    return f\"70 Degree Fahrenheit in {city}\"\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> str:\n",
    "    return f\"Sunny in {city}\"\n",
    "\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name = \"Weather Agent\",\n",
    "    instructions = \"You are a weather agent. Your job is to get the weather of a city and temperature.\",\n",
    "    model = \"gpt-4o-mini\",\n",
    "    tools=[getTemperature, get_weather]\n",
    ")\n",
    "weather_agent_result = await Runner.run(weather_agent, \"Get the weather and current temperature of Karachi.\")\n",
    "print(weather_agent_result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handoffs \n",
    "Handoffs allow an agent to delegate tasks to another agent\n",
    "This is particularly useful in scenarios where different agents specialize in distinct areas. For example, a customer support app might have agents that each specifically handle tasks like order status, refunds, FAQs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Outline for a Tutorial on Python Lists\n",
      "\n",
      "## I. Introduction\n",
      "   A. What are Lists?\n",
      "   B. Importance of Lists in Python\n",
      "   C. Overview of Tutorial Structure\n",
      "\n",
      "## II. Creating Lists\n",
      "   A. Syntax for List Creation\n",
      "   B. Different Ways to Create Lists\n",
      "      1. Using Square Brackets\n",
      "      2. Using the `list()` Function\n",
      "      3. Creating an Empty List\n",
      "   C. Example: Basic List Creation\n",
      "\n",
      "## III. Accessing List Elements\n",
      "   A. Indexing in Python Lists\n",
      "      1. Positive Indexing\n",
      "      2. Negative Indexing\n",
      "   B. Slicing Lists\n",
      "      1. Basic Slicing\n",
      "      2. Advanced Slicing Techniques\n",
      "   C. Example: Accessing Elements in a List\n",
      "\n",
      "## IV. Modifying Lists\n",
      "   A. Adding Elements\n",
      "      1. Using `append()`\n",
      "      2. Using `insert()`\n",
      "      3. Using `extend()`\n",
      "   B. Removing Elements\n",
      "      1. Using `remove()`\n",
      "      2. Using `pop()`\n",
      "      3. Using `clear()`\n",
      "   C. Example: Modifying List Contents\n",
      "\n",
      "## V. List Methods and Functions\n",
      "   A. Common List Methods\n",
      "      1. `sort()`\n",
      "      2. `reverse()`\n",
      "      3. `count()`\n",
      "      4. `index()`\n",
      "   B. Useful Built-in Functions\n",
      "      1. `len()`\n",
      "      2. `min()` and `max()`\n",
      "      3. `sum()`\n",
      "   C. Example: Using List Methods\n",
      "\n",
      "## VI. List Comprehensions\n",
      "   A. Introduction to List Comprehensions\n",
      "   B. Syntax and Examples\n",
      "   C. Advantages of Using List Comprehensions\n",
      "\n",
      "## VII. Nested Lists\n",
      "   A. What are Nested Lists?\n",
      "   B. Accessing Elements in Nested Lists\n",
      "   C. Example: Working with Nested Lists\n",
      "\n",
      "## VIII. Practical Applications of Lists\n",
      "   A. Use Cases in Data Management\n",
      "   B. Implementing Algorithms with Lists\n",
      "   C. Example: Building a Simple Python Program Using Lists\n",
      "\n",
      "## IX. Summary\n",
      "   A. Recap of Key Points\n",
      "   B. Further Resources for Learning\n",
      "   C. Encouragement to Practice\n",
      "\n",
      "## X. Q&A Section (Optional)\n",
      "   A. Address Common Questions\n",
      "   B. Provide Answers to Frequently Asked Questions\n",
      "\n",
      "## XI. Conclusion\n",
      "   A. Final Thoughts on Python Lists\n",
      "   B. Encouragement to Explore More Python Topics\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, function_tool\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Tutorial(BaseModel):\n",
    "    outline: str\n",
    "    tutorial: str\n",
    "\n",
    "Tutorial_agent = Agent(\n",
    "    name = \"Tutorial Agent\",\n",
    "     handoff_description = \"use for generating tutorials based on given outline and topic\",\n",
    "    instructions = (\"You are a tutorial agent. Your job is to create a tutorial for a given topic.\", \"give detail tutorial on every topic of the outline also add comments explanation and code snippets.\"),\n",
    "    model = \"gpt-4o-mini\",\n",
    "    output_type = Tutorial\n",
    ")\n",
    "\n",
    "\n",
    "outline_agent = Agent(\n",
    "    name = \"Outline Agent\",\n",
    "    instructions = \"You are an outline agent. Your job is to create an outline for a given topic.\",\n",
    "    model = \"gpt-4o-mini\",\n",
    "    handoffs = [Tutorial_agent]\n",
    ")\n",
    "\n",
    "tutorial_response = await Runner.run(outline_agent, \"Create an outline for a tutorial on Python lists.\")\n",
    "print(tutorial_response.final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi AI Agents using handoffs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handing off to history tutor agent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The first president of the United States was George Washington. He served from April 30, 1789, to March 4, 1797. Washington is often called the \"Father of His Country\" for his pivotal role in leading the nation during the founding years.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agents import Agent, Runner, function_tool, handoff, RunContextWrapper\n",
    "\n",
    "\n",
    "math_tutor_agent = Agent(\n",
    "    name = \"Math Tutor Agent\",\n",
    "    handoff_description = \"Specialized agent for solving math problems\",\n",
    "    instructions = \"You are a math tutor. Your job is to solve math problems.\",\n",
    "    model = \"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "def math_tutor_handoff(context: RunContextWrapper[None]):\n",
    "    print('Handing off to math tutor agent')\n",
    "    \n",
    "\n",
    "history_tutor_agent = Agent(\n",
    "    name = \"History Tutor Agent\",\n",
    "    handoff_description = \"Specialized agent for solving history problems\",\n",
    "    instructions = \"You are a history tutor. Your job is to solve history problems.\",\n",
    "    model = \"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "def history_tutor_handoff(context: RunContextWrapper[None]):\n",
    "    print('Handing off to history tutor agent')\n",
    "\n",
    "\n",
    "multi_agent_team = Agent(\n",
    "    name = \"Multi-Agent Team\",\n",
    "    instructions = \"You are a multi-agent team. Your job is to solve problems using specialized agents.\",\n",
    "    handoffs=[handoff(math_tutor_agent, on_handoff=math_tutor_handoff), handoff(history_tutor_agent, on_handoff=history_tutor_handoff)],\n",
    ")\n",
    "\n",
    "result = await Runner.run(multi_agent_team, \"who is the first president of USA\")\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw response events\n",
    "\n",
    "RawResponsesStreamEvent are raw events passed directly from the LLM. They are in OpenAI Responses API format, which means each event has a type (like response.created, response.output_text.delta, etc) and data. These events are useful if you want to stream response messages to the user as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are five jokes for you:\n",
      "\n",
      "1. **Why don't scientists trust atoms?**  \n",
      "   Because they make up everything!\n",
      "\n",
      "2. **What do you call fake spaghetti?**  \n",
      "   An impasta!\n",
      "\n",
      "3. **Why did the scarecrow win an award?**  \n",
      "   Because he was outstanding in his field!\n",
      "\n",
      "4. **How do you organize a space party?**  \n",
      "   You planet!\n",
      "\n",
      "5. **Why did the bicycle fall over?**  \n",
      "   Because it was two-tired! \n",
      "\n",
      "Hope these made you smile!"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "from agents import Agent, Runner\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"You are a helpful assistant.\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )\n",
    "\n",
    "    result = Runner.run_streamed(agent, input=\"Please tell me 5 jokes.\")\n",
    "    async for event in result.stream_events():\n",
    "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "\n",
    "# In a Jupyter notebook, use await instead of asyncio.run()\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardrails \n",
    "\n",
    "Guardrails run in parallel to your agents, enabling you to do checks and validations of user input. For example, imagine you have an agent that uses a very smart (and hence slow/expensive) model to help with customer requests. You wouldn't want malicious users to ask the model to help them with their math homework. So, you can run a guardrail with a fast/cheap model. If the guardrail detects malicious usage, it can immediately raise an error, which stops the expensive model from running and saves you time/money.\n",
    "\n",
    "There are two kinds of guardrails:\n",
    "\n",
    "1. Input guardrails run on the initial user input\n",
    "2. Output guardrails run on the final agent output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input guardrails\n",
    "\n",
    "Input guardrails run in 3 steps:\n",
    "\n",
    "1. First, the guardrail receives the same input passed to the agent.\n",
    "2. Next, the guardrail function runs to produce a GuardrailFunctionOutput, which is then wrapped in an InputGuardrailResult\n",
    "3. Finally, we check if .tripwire_triggered is true. If true, an InputGuardrailTripwireTriggered exception is raised, so you can appropriately respond to the user or handle the exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from agents import (\n",
    "    Agent,\n",
    "    GuardrailFunctionOutput,\n",
    "    InputGuardrailTripwireTriggered,\n",
    "    RunContextWrapper,\n",
    "    Runner,\n",
    "    TResponseInputItem,\n",
    "    input_guardrail,\n",
    ")\n",
    "\n",
    "class MathHomeworkOutput(BaseModel):\n",
    "    is_math_homework: bool\n",
    "    reasoning: str\n",
    "\n",
    "# We'll use this agent in our guardrail function.\n",
    "guardrail_agent = Agent( \n",
    "    name=\"Guardrail check\",\n",
    "    instructions=\"Check if the user is asking you to do their customer support related query or out of context.\",\n",
    "    output_type=MathHomeworkOutput,\n",
    ")\n",
    "\n",
    "\n",
    "# This is the guardrail function that receives the agent's input/context, and returns the result.\n",
    "@input_guardrail\n",
    "async def math_guardrail( \n",
    "    ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]\n",
    ") -> GuardrailFunctionOutput:\n",
    "    result = await Runner.run(guardrail_agent, input, context=ctx.context)\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=result.final_output, \n",
    "        tripwire_triggered=result.final_output.is_math_homework,\n",
    "    )\n",
    "\n",
    "\n",
    "agent = Agent(  \n",
    "    name=\"Customer support agent\",\n",
    "    instructions=\"you are only design to answer customer support related queries. if user ask me to do math homework, please refuse.\",\n",
    "    input_guardrails=[math_guardrail],\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    # This should trip the guardrail\n",
    "    try:\n",
    "        result = await Runner.run(agent, \"how to cook biryani\")\n",
    "        print(\"Guardrail didn't trip - this is unexpected\")\n",
    "        print(result.final_output)\n",
    "\n",
    "    except InputGuardrailTripwireTriggered:\n",
    "        print(\"Math homework guardrail tripped\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output guardrails\n",
    "\n",
    "Output guardrails run in 3 steps:\n",
    "\n",
    "1. First, the guardrail receives the same input passed to the agent.\n",
    "2. Next, the guardrail function runs to produce a GuardrailFunctionOutput, which is then wrapped in an OutputGuardrailResult\n",
    "3. Finally, we check if .tripwire_triggered is true. If true, an OutputGuardrailTripwireTriggered exception is raised, so you can appropriately respond to the user or handle the exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from agents import (\n",
    "    Agent,\n",
    "    GuardrailFunctionOutput,\n",
    "    OutputGuardrailTripwireTriggered,\n",
    "    RunContextWrapper,\n",
    "    Runner,\n",
    "    output_guardrail,\n",
    ")\n",
    "class MessageOutput(BaseModel): \n",
    "    response: str\n",
    "\n",
    "class MathOutput(BaseModel): \n",
    "    reasoning: str\n",
    "    is_math: bool\n",
    "\n",
    "guardrail_agent = Agent(\n",
    "    name=\"Guardrail check\",\n",
    "    instructions=\"Check if the output includes any math.\",\n",
    "    output_type=MathOutput,\n",
    ")\n",
    "\n",
    "@output_guardrail\n",
    "async def math_guardrail(  \n",
    "    ctx: RunContextWrapper, agent: Agent, output: MessageOutput\n",
    ") -> GuardrailFunctionOutput:\n",
    "    result = await Runner.run(guardrail_agent, output.response, context=ctx.context)\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=result.final_output,\n",
    "        tripwire_triggered=result.final_output.is_math,\n",
    "    )\n",
    "\n",
    "agent = Agent( \n",
    "    name=\"Customer support agent\",\n",
    "    instructions=\"You are a customer support agent. You help customers with their questions.\",\n",
    "    output_guardrails=[math_guardrail],\n",
    "    output_type=MessageOutput,\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    # This should trip the guardrail\n",
    "    try:\n",
    "        await Runner.run(agent, \"Hello, can you help me solve for x: 2x + 3 = 11?\")\n",
    "        print(\"Guardrail didn't trip - this is unexpected\")\n",
    "\n",
    "    except OutputGuardrailTripwireTriggered:\n",
    "        print(\"Math output guardrail tripped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Conversation AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are chatting with Alice, Type 'exit' to end the conversation.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, TResponseInputItem\n",
    "\n",
    "simple_agent = Agent(\n",
    "    name=\"Simple Agent\",\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "conversation_history: list[TResponseInputItem] = []\n",
    "print(\"You are chatting with Alice, Type 'exit' to end the conversation.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    print(\"User:\", user_input)\n",
    "    result = await Runner.run(simple_agent, conversation_history)\n",
    "    print(f\"Alice: {result.final_output}\")\n",
    "    conversation_history = result.to_input_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from agents import Agent, RunContextWrapper, Runner, function_tool\n",
    "\n",
    "@dataclass\n",
    "class UserInfo:  \n",
    "    name: str\n",
    "    uid: int\n",
    "\n",
    "@function_tool\n",
    "async def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -> str:  \n",
    "    return f\"User {wrapper.context.name} is 47 years old\"\n",
    "\n",
    "async def main():\n",
    "    user_info = UserInfo(name=\"John\", uid=123)\n",
    "\n",
    "    agent = Agent[UserInfo](  \n",
    "        name=\"Assistant\",\n",
    "        tools=[fetch_user_age],\n",
    "    )\n",
    "\n",
    "    result = await Runner.run(  \n",
    "        starting_agent=agent,\n",
    "        input=\"What is the age of the user?\",\n",
    "        context=user_info,\n",
    "    )\n",
    "\n",
    "    print(result.final_output)  \n",
    "    # The user John is 47 years old.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"openai-agents[viz]\" -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
